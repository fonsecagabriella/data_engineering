{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0efb6c",
   "metadata": {},
   "source": [
    "# Spark & Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3307b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db9348",
   "metadata": {},
   "source": [
    "First we will move our local data to a bucket in GCS.\n",
    "You need authenticate:\n",
    "\n",
    "`gcloud auth activate-service-account --key-file KEY.json`\n",
    "\n",
    "Then navigate to the folder where the .parquet files are, to be able to copy them to the bucket:\n",
    "`gsutil -m cp -r pq/ gs://BUCKET/pq`\n",
    "\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `m`, multithread, since we're addting a lot of files, runs process in paralel\n",
    "\n",
    "- `cp`, copy\n",
    "\n",
    "- `-r` recursive, because we're copying multiple files\n",
    "\n",
    "----\n",
    "\n",
    "## Reading files in the cloud\n",
    "\n",
    "**Dowload Hadoop connector**\n",
    "\n",
    "\n",
    "To be able to read files that are stored in the bucket locally we need to download the GCS connector *Hadoop*.\n",
    "\n",
    "- Create a folder `lib` and run the following:\n",
    "\n",
    "`gsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar gcs-connector-hadoop3-2.2.5.jar` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f0ddbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = 'peppy.json'\n",
    "bucket_name = \"imgabi-zoomcamp-kestra\"\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "# line 03: location of jar file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b83404e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "# here we have:\n",
    "# we you seee filesystem is gs\n",
    "# you need to use Google Credentials\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4713e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ee1eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if it works, you can read the data\n",
    "# USE\n",
    "\n",
    "df_green = spark.read.parquet(f'gs://{bucket_name}/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "104b40ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1734051"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f56a885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ec0da",
   "metadata": {},
   "source": [
    "## Create a Local Spark Cluster\n",
    "\n",
    "First you need to find the location in which Spark is installed.\n",
    "Since we installed it with brew, run:\n",
    "\n",
    "`brew --prefix apache-spark`\n",
    "\n",
    "In my case, I can navigate to this folder: `/usr/local/Cellar/apache-spark/3.5.4/`.\n",
    "\n",
    "From there, run:\n",
    "\n",
    "`./sbin/start-master.sh`\n",
    "\n",
    "Then you will be able to open Spark dashboard at localhost:8080.\n",
    "\n",
    "We will see that there are no workers listed. To create one, we can do the following:\n",
    "\n",
    "`./sbin/start-worker.sh spark://<master-spark-URL>`\n",
    "\n",
    "<img src=\"./img/spark-master.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe959dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/24 18:21:46 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.178.241: Command exited with code 143\n",
      "25/02/24 18:22:05 WARN StandaloneAppClient$ClientEndpoint: Connection to Gabis-iMac-Pro.local:7077 failed; waiting for master to reconnect...\n",
      "25/02/24 18:22:05 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n"
     ]
    }
   ],
   "source": [
    "# now we can connect to the SparkMaster instead of using local[*]\n",
    "# to do that we need to change the configuration of the SparkConf\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start the session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://localhost:7077\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa895d6",
   "metadata": {},
   "source": [
    "We can see now we have one worker, and one application (which we just created above).\n",
    "\n",
    "<img src=\"./img/spark-master-2.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33228f",
   "metadata": {},
   "source": [
    "## Creating a Spark Job\n",
    "\n",
    "We will reuse the code we created earlier, with the taxi schema. \n",
    "The code is adapted so it can receive parameters via de command line.\n",
    "It can be found at [spark_parquet.py](./spark_parquet.py).\n",
    "\n",
    "This is how you would run it mannually\n",
    "\n",
    "```python\n",
    "python spark_parquet.py \\\n",
    "    --input_green=\"data/pq/green/2020/*\" \\\n",
    "    --input_yellow=\"data/pq/yellow/2020/*\" \\\n",
    "    --output=\"data/report-2020\"\n",
    "```\n",
    "\n",
    "\n",
    "However, we can use `spark-submit`to be able to send the job directly. We also identify the master here, and run it in the shell:\n",
    "\n",
    "```bash\n",
    "spark-submit \\\n",
    "    --master=\"spark://<URL>\" \\\n",
    "    my_script.py \\\n",
    "        --input_green=data/pq/green/2020/*/ \\\n",
    "        --input_yellow=data/pq/yellow/2020/*/ \\\n",
    "        --output=data/report-2020\n",
    "```\n",
    "\n",
    "Once we are done with the work we need to stop the worker and the master:\n",
    "\n",
    "`./sbin/stop-worker.sh`\n",
    "\n",
    "`./sbin/stop-master.sh`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f81d36",
   "metadata": {},
   "source": [
    "## Setting up a Dataproc Cluster\n",
    "\n",
    "- Go to GCS, and select `dataproc cluster` (you might need to enable it, if it's the first time you're using it)\n",
    "\n",
    "- Create a new cluster, with default settings. Make sure it's on the same location as your bucket.\n",
    "\n",
    "- Via the terminal, you can copy the code [spark_parquet.py](./spark_parquet.py) to your bucket:\n",
    "\n",
    "`gsutil cp spark_parquet.py gs://YOUR_BUCKET/code/spark_parquet.py`\n",
    "\n",
    "- Click on the cluster you created to submit a new job.\n",
    "Remember we need to submit the arguments. This is done by adding one argument at a time. \n",
    "We also replace the values with our bucket name:\n",
    "\n",
    "```\n",
    "        --input_green=gs://BUCKET/pq/green/2020/*/ \\\n",
    "        --input_yellow=gs://BUCKET/pq/yellow/2020/*/ \\\n",
    "        --output=gs://BUCKET/report-2020\n",
    "```\n",
    "\n",
    "<img src=\"./img/spark-job.png\" width=\"70%\">\n",
    "\n",
    "Once the job finishes running, we can see that there is a folder inside the BUCKET with the report-2020.\n",
    "\n",
    "\n",
    "#### Submitting job via SDK\n",
    "\n",
    "You can also submit jobs with the SDK. \n",
    "**NOTE:** Make sure you have enough permissions in your service account (*simple solution: Dataproc Administrator*)\n",
    "\n",
    "```bash\n",
    "gcloud dataproc jobs submit pyspark \\\n",
    "    --cluster=<your-cluster-name> \\\n",
    "    --region=<region-of-your-cluster> \\\n",
    "    gs://<url-of-your-script-in-bucket> \\\n",
    "    -- \\\n",
    "        --param1=<your-param-value> \\\n",
    "        --param2=<your-param-value>\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
