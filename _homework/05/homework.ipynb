{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbedb5b",
   "metadata": {},
   "source": [
    "# Homework - Week 05 - Batch Processing\n",
    "\n",
    "```bash\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
    "```\n",
    "\n",
    "\n",
    "## Question 1: Install Spark and PySpark\n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "\n",
    "What's the output?\n",
    ">> '3.5.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bc6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4a0f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/25 10:50:41 WARN Utils: Your hostname, Gabis-iMac-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.178.241 instead (on interface en1)\n",
      "25/02/25 10:50:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/25 10:50:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3e4c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3445691",
   "metadata": {},
   "source": [
    "## Question 2: Yellow October 2024\n",
    "\n",
    "Read the October 2024 Yellow into a Spark Dataframe.\n",
    "\n",
    "Repartition the Dataframe to 4 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "> 25MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1fe2f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_oct_24 = spark.read.parquet('./yellow_tripdata_2024-10.parquet') # read the yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c42cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow_oct_24.printSchema() # print the schema of the data, just to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a939d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# repartion the data in 4 partitions and save it in the output path\n",
    "df_yellow_oct_24 \\\n",
    "        .repartition(4) \\\n",
    "        .write.parquet(\"output_yellow_oct_24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe2c90",
   "metadata": {},
   "source": [
    "We can check manually or via Spark:\n",
    "\n",
    "<img src=\"./question-2.png\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b01d2f",
   "metadata": {},
   "source": [
    "## Question 3: Count records \n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n",
    "\n",
    "> 125,567\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7489aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128893"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_yellow_oct_24 = spark.read.parquet(\"yellow_tripdata_2024-10.parquet\") # read the data from the output path\n",
    "\n",
    "df_yellow_oct_24 \\\n",
    "    .withColumn('pickup_date', F.to_date(df_yellow_oct_24.tpep_pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2024-10-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f533b",
   "metadata": {},
   "source": [
    "## Question 4: Longest trip\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "> 162\n",
    "\n",
    "*Explanation:*\n",
    "- `F.unix_timestamp()`: Converts the timestamp column into seconds.\n",
    "- Duration Calculation: Subtracts drop-off time from pick-up time, and divides by 3600 to ge the value in hours\n",
    "- `F.agg()`: Aggregates with max() while renaming the output column (alias()).\n",
    "- `orderBy()`: Sorts by maximum duration in descending order.\n",
    "- `limit()`: Restricts output to the top 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "279d9161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|max_duration_hours|\n",
      "+-----------+------------------+\n",
      "| 2024-10-16|162.61777777777777|\n",
      "| 2024-10-03|           143.325|\n",
      "| 2024-10-22|137.76055555555556|\n",
      "| 2024-10-18|114.83472222222223|\n",
      "| 2024-10-21| 89.89833333333333|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_yellow_oct_24 \\\n",
    "    .withColumn('duration_hours', \n",
    "                (F.unix_timestamp(df_yellow_oct_24.tpep_dropoff_datetime) - \n",
    "                 F.unix_timestamp(df_yellow_oct_24.tpep_pickup_datetime)) / 3600) \\\n",
    "    .withColumn('pickup_date', F.to_date(df_yellow_oct_24.tpep_pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "    .agg(F.max('duration_hours').alias('max_duration_hours')) \\\n",
    "    .orderBy('max_duration_hours', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915096b",
   "metadata": {},
   "source": [
    "## Question 5: User Interface\n",
    "\n",
    "Sparkâ€™s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "> 4040\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f1431",
   "metadata": {},
   "source": [
    "## Question 6: Least frequent pickup location zone\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark:\n",
    "\n",
    "```bash\n",
    "wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
    "```\n",
    "\n",
    "Using the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "> Governor's Island/Ellis Island/Liberty Island\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cf17a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the location data\n",
    "df_location = spark.read.csv(\"taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_location.createOrReplaceTempView(\"zone_lookup\") # create a temporary view for the location data\n",
    "df_yellow_oct_24.createOrReplaceTempView(\"yellow_trip_data\") # create a temporary view for the yellow data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f9fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|                Zone|pickup_count|\n",
      "+--------------------+------------+\n",
      "|Governor's Island...|           1|\n",
      "|       Rikers Island|           2|\n",
      "|       Arden Heights|           2|\n",
      "|         Jamaica Bay|           3|\n",
      "| Green-Wood Cemetery|           3|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Query to Find the Location with the Least Pickups\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT z.Zone, COUNT(y.PULocationID) AS pickup_count\n",
    "    FROM yellow_trip_data y\n",
    "    JOIN zone_lookup z ON y.PULocationID = z.LocationID\n",
    "    GROUP BY z.Zone\n",
    "    ORDER BY pickup_count ASC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "# Display the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f54f555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|                Zone|pickup_count|\n",
      "+--------------------+------------+\n",
      "|Governor's Island...|           1|\n",
      "|       Rikers Island|           2|\n",
      "|       Arden Heights|           2|\n",
      "|         Jamaica Bay|           3|\n",
      "| Green-Wood Cemetery|           3|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE SOLUTION \n",
    "# Step 1: Perform the join between yellow_trip_data and zone_lookup\n",
    "joined_df = df_yellow_oct_24.join(df_location, df_yellow_oct_24.PULocationID == df_location.LocationID, \"inner\")\n",
    "\n",
    "# Step 2: Count pickups by zone\n",
    "pickup_counts = joined_df.groupBy(\"Zone\").agg(F.count(\"PULocationID\").alias(\"pickup_count\"))\n",
    "\n",
    "# Step 3: Find the 5 locations with the least pickups\n",
    "result = pickup_counts.orderBy(\"pickup_count\").limit(5)\n",
    "\n",
    "# Step 4: Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f02ffbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop connection\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
